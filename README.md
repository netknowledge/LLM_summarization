# Abstract 
The ability to provide a concise and informative summary is common. Using a large sample
of summaries authored by researchers, we evaluate the capability of LLMs in generating a
summary of a paper based on its abstract. We find that LLMs consistently outperform human
experts in extreme summarization, producing summaries that better preserve key information
from the original text. Through extensive evaluation using both reference-based metrics and
downstream prediction tasks, we demonstrate that summaries generated by LLMs enable
superior performance in title matching and topic prediction compared to human-authored
summaries. Interestingly, both human annotators and LLMs exhibit a shared citation bias,
performing better when summarizing highly-cited papers. Our findings highlight the potential
of LLMs as effective tools for scientific summarization while raising important considerations
about their tendency to reinforce existing Matthew effect.

# File location

## Data
1. **LLM-generated summaries**: `data/paper_html_10.1038/abs_annotation/generated_annotations`

2. **Human-authored summaries**: `data/paper_html_10.1038/abs_annotation/test.tsv`

3. **The pre-packed dataset**: `data/dataset.csv`

## Experiment code

1. **LLMs are more extractive summarizer**: `evaluation/ref_based`
2. **LLMs outperform human consistently**: `evaluation/predict_task`
3. **LLMs mirror humanâ€™s citation bias**: `bias`
